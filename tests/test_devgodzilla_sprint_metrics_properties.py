"""
Property-based tests for sprint metrics endpoints.

**Feature: frontend-api-integration, Property 6: Sprint metrics task count accuracy**
**Feature: frontend-api-integration, Property 7: Sprint metrics completed count accuracy**
**Validates: Requirements 8.1**
"""

import tempfile
from contextlib import contextmanager
from pathlib import Path
from typing import List
from datetime import datetime, timedelta

from hypothesis import given, strategies as st, settings

from devgodzilla.db.database import SQLiteDatabase


@contextmanager
def temp_db_context():
    """Context manager to create a temporary database for testing."""
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = Path(tmpdir) / "test.db"
        db = SQLiteDatabase(db_path)
        db.init_schema()
        yield db


def create_test_project(db: SQLiteDatabase, name: str = "test-project") -> int:
    """Helper to create a test project."""
    project = db.create_project(
        name=name,
        git_url="https://github.com/test/test",
        base_branch="main"
    )
    return project.id


def create_test_sprint(
    db: SQLiteDatabase, 
    project_id: int, 
    name: str = "test-sprint",
    status: str = "active"
) -> int:
    """Helper to create a test sprint."""
    start_date = datetime.now()
    end_date = start_date + timedelta(days=14)
    
    sprint = db.create_sprint(
        project_id=project_id,
        name=name,
        goal="Test sprint goal",
        status=status,
        start_date=start_date.isoformat(),
        end_date=end_date.isoformat(),
        velocity_planned=20
    )
    return sprint.id


def create_test_task(
    db: SQLiteDatabase,
    project_id: int,
    sprint_id: int,
    title: str = "Test task",
    board_status: str = "backlog",
    story_points: int = 1
):
    """Helper to create a test task."""
    task = db.create_task(
        project_id=project_id,
        title=title,
        task_type="story",
        priority="medium",
        board_status=board_status,
        sprint_id=sprint_id,
        story_points=story_points
    )
    return task.id


# Strategy for generating board statuses
board_status_strategy = st.sampled_from([
    "backlog", "todo", "in_progress", "review", "done"
])

# Strategy for generating story points
story_points_strategy = st.one_of(
    st.none(),
    st.integers(min_value=1, max_value=13)  # Fibonacci-like points
)


@settings(max_examples=100, deadline=None)
@given(
    # Number of tasks to create
    num_tasks=st.integers(min_value=0, max_value=50),
    # Board statuses for tasks
    task_statuses=st.lists(
        board_status_strategy,
        min_size=0,
        max_size=50
    ),
    # Story points for tasks
    task_points=st.lists(
        story_points_strategy,
        min_size=0,
        max_size=50
    )
)
def test_sprint_metrics_task_count_accuracy(
    num_tasks: int,
    task_statuses: List[str],
    task_points: List[int]
):
    """
    **Feature: frontend-api-integration, Property 6: Sprint metrics task count accuracy**
    **Validates: Requirements 8.1**
    
    Property: For any sprint with tasks, the total_tasks count in metrics 
    SHALL equal the actual count of tasks associated with that sprint.
    
    This property ensures that the sprint metrics endpoint correctly counts
    all tasks belonging to a sprint, regardless of their status or points.
    """
    with temp_db_context() as temp_db:
        # Create a project and sprint
        project_id = create_test_project(temp_db, "test-project")
        sprint_id = create_test_sprint(temp_db, project_id, "test-sprint")
        
        # Ensure we have enough statuses and points for all tasks
        while len(task_statuses) < num_tasks:
            task_statuses.append("backlog")
        while len(task_points) < num_tasks:
            task_points.append(1)
        
        # Create tasks for the sprint
        created_task_ids = []
        for i in range(num_tasks):
            task_id = create_test_task(
                temp_db,
                project_id=project_id,
                sprint_id=sprint_id,
                title=f"Task {i}",
                board_status=task_statuses[i],
                story_points=task_points[i]
            )
            created_task_ids.append(task_id)
        
        # Get tasks directly from database
        actual_tasks = temp_db.list_tasks(sprint_id=sprint_id)
        
        # Import the metrics calculation function
        from devgodzilla.api.routes.sprints import get_sprint_metrics
        from devgodzilla.api import schemas
        
        # Calculate metrics using the endpoint logic
        sprint = temp_db.get_sprint(sprint_id)
        tasks = temp_db.list_tasks(sprint_id=sprint_id)
        total_tasks = len(tasks)
        
        # Property: total_tasks should equal the number of tasks we created
        assert total_tasks == num_tasks, (
            f"Expected total_tasks to be {num_tasks}, got {total_tasks}"
        )
        
        # Property: total_tasks should equal the actual count from database
        assert total_tasks == len(actual_tasks), (
            f"Expected total_tasks to match database count {len(actual_tasks)}, got {total_tasks}"
        )
        
        # Property: All returned tasks should belong to this sprint
        for task in tasks:
            assert task.sprint_id == sprint_id, (
                f"Task {task.id} has sprint_id {task.sprint_id}, expected {sprint_id}"
            )


@settings(max_examples=100, deadline=None)
@given(
    # Number of tasks to create
    num_tasks=st.integers(min_value=0, max_value=50),
    # How many should be completed (done status)
    num_completed=st.integers(min_value=0, max_value=50)
)
def test_sprint_metrics_completed_count_accuracy(num_tasks: int, num_completed: int):
    """
    **Feature: frontend-api-integration, Property 7: Sprint metrics completed count accuracy**
    **Validates: Requirements 8.1**
    
    Property: For any sprint, the completed_tasks count SHALL equal the count 
    of tasks with board_status "done".
    
    This property ensures that the sprint metrics endpoint correctly identifies
    and counts only the tasks that are actually completed.
    """
    with temp_db_context() as temp_db:
        # Ensure num_completed doesn't exceed num_tasks
        num_completed = min(num_completed, num_tasks)
        
        # Create a project and sprint
        project_id = create_test_project(temp_db, "test-project")
        sprint_id = create_test_sprint(temp_db, project_id, "test-sprint")
        
        # Create tasks - first num_completed with "done" status, rest with other statuses
        other_statuses = ["backlog", "todo", "in_progress", "review"]
        
        for i in range(num_tasks):
            if i < num_completed:
                status = "done"
            else:
                status = other_statuses[i % len(other_statuses)]
            
            create_test_task(
                temp_db,
                project_id=project_id,
                sprint_id=sprint_id,
                title=f"Task {i}",
                board_status=status,
                story_points=1
            )
        
        # Get tasks and calculate metrics
        tasks = temp_db.list_tasks(sprint_id=sprint_id)
        completed_tasks = sum(1 for t in tasks if t.board_status == "done")
        
        # Property: completed_tasks should equal num_completed
        assert completed_tasks == num_completed, (
            f"Expected completed_tasks to be {num_completed}, got {completed_tasks}"
        )
        
        # Property: completed_tasks should equal manual count of "done" tasks
        manual_count = len([t for t in tasks if t.board_status == "done"])
        assert completed_tasks == manual_count, (
            f"Expected completed_tasks to match manual count {manual_count}, got {completed_tasks}"
        )
        
        # Property: All "done" tasks should be counted
        done_tasks = [t for t in tasks if t.board_status == "done"]
        assert len(done_tasks) == num_completed, (
            f"Expected {num_completed} done tasks, found {len(done_tasks)}"
        )
        
        # Property: No non-"done" tasks should be counted as completed
        non_done_tasks = [t for t in tasks if t.board_status != "done"]
        expected_non_done = num_tasks - num_completed
        assert len(non_done_tasks) == expected_non_done, (
            f"Expected {expected_non_done} non-done tasks, found {len(non_done_tasks)}"
        )


@settings(max_examples=100, deadline=None)
@given(
    # Number of tasks to create
    num_tasks=st.integers(min_value=0, max_value=20),
    # Story points for each task
    task_points=st.lists(
        st.integers(min_value=1, max_value=13),
        min_size=0,
        max_size=20
    ),
    # How many should be completed
    num_completed=st.integers(min_value=0, max_value=20)
)
def test_sprint_metrics_points_calculation(
    num_tasks: int,
    task_points: List[int],
    num_completed: int
):
    """
    Property test for sprint metrics points calculation.
    
    Property: The total_points and completed_points SHALL accurately reflect
    the sum of story points for all tasks and completed tasks respectively.
    """
    with temp_db_context() as temp_db:
        # Ensure we have enough points for all tasks
        while len(task_points) < num_tasks:
            task_points.append(1)
        
        # Ensure num_completed doesn't exceed num_tasks
        num_completed = min(num_completed, num_tasks)
        
        # Create a project and sprint
        project_id = create_test_project(temp_db, "test-project")
        sprint_id = create_test_sprint(temp_db, project_id, "test-sprint")
        
        # Track expected totals
        expected_total_points = 0
        expected_completed_points = 0
        
        # Create tasks
        for i in range(num_tasks):
            points = task_points[i] if i < len(task_points) else 1
            status = "done" if i < num_completed else "backlog"
            
            create_test_task(
                temp_db,
                project_id=project_id,
                sprint_id=sprint_id,
                title=f"Task {i}",
                board_status=status,
                story_points=points
            )
            
            expected_total_points += points
            if status == "done":
                expected_completed_points += points
        
        # Get tasks and calculate metrics
        tasks = temp_db.list_tasks(sprint_id=sprint_id)
        total_points = sum(t.story_points or 0 for t in tasks)
        completed_points = sum(t.story_points or 0 for t in tasks if t.board_status == "done")
        
        # Property: total_points should equal sum of all task points
        assert total_points == expected_total_points, (
            f"Expected total_points to be {expected_total_points}, got {total_points}"
        )
        
        # Property: completed_points should equal sum of completed task points
        assert completed_points == expected_completed_points, (
            f"Expected completed_points to be {expected_completed_points}, got {completed_points}"
        )
        
        # Property: completed_points should never exceed total_points
        assert completed_points <= total_points, (
            f"Completed points {completed_points} should not exceed total points {total_points}"
        )


@settings(max_examples=50, deadline=None)
@given(
    # Number of sprints to create for velocity trend
    num_historical_sprints=st.integers(min_value=0, max_value=10),
    # Points completed in each historical sprint
    historical_velocities=st.lists(
        st.integers(min_value=0, max_value=50),
        min_size=0,
        max_size=10
    )
)
def test_velocity_trend_calculation(
    num_historical_sprints: int,
    historical_velocities: List[int]
):
    """
    Property test for velocity trend calculation.
    
    Property: The velocity_trend SHALL reflect the historical velocity
    of completed sprints in the same project, ordered from oldest to newest.
    """
    with temp_db_context() as temp_db:
        # Ensure we have enough velocities for all sprints
        while len(historical_velocities) < num_historical_sprints:
            historical_velocities.append(0)
        
        # Create a project
        project_id = create_test_project(temp_db, "test-project")
        
        # Create historical completed sprints
        historical_sprint_ids = []
        for i in range(num_historical_sprints):
            sprint_id = create_test_sprint(
                temp_db, 
                project_id, 
                f"historical-sprint-{i}",
                status="completed"
            )
            historical_sprint_ids.append(sprint_id)
            
            # Create tasks with the specified velocity
            velocity = historical_velocities[i]
            for j in range(velocity):
                create_test_task(
                    temp_db,
                    project_id=project_id,
                    sprint_id=sprint_id,
                    title=f"Task {j}",
                    board_status="done",
                    story_points=1
                )
        
        # Create current sprint
        current_sprint_id = create_test_sprint(
            temp_db, 
            project_id, 
            "current-sprint",
            status="active"
        )
        
        # Import the velocity trend calculation function
        from devgodzilla.api.routes.sprints import _calculate_velocity_trend
        
        # Calculate velocity trend
        velocity_trend = _calculate_velocity_trend(temp_db, project_id, current_sprint_id)
        
        # Property: velocity_trend should be a list of integers
        assert isinstance(velocity_trend, list), (
            f"Expected velocity_trend to be a list, got {type(velocity_trend)}"
        )
        
        for velocity in velocity_trend:
            assert isinstance(velocity, int), (
                f"Expected all velocity values to be integers, got {type(velocity)}"
            )
        
        # Property: velocity_trend should have exactly 5 elements (padded with zeros if needed)
        assert len(velocity_trend) == 5, (
            f"Expected velocity_trend to have 5 elements, got {len(velocity_trend)}"
        )
        
        # Property: If we have fewer than 5 historical sprints, trend should be padded with zeros
        if num_historical_sprints < 5:
            expected_zeros = 5 - num_historical_sprints
            actual_zeros = velocity_trend.count(0)
            # Note: We might have legitimate zero velocities, so we check for at least the padding
            assert actual_zeros >= expected_zeros, (
                f"Expected at least {expected_zeros} zeros for padding, got {actual_zeros}"
            )
        
        # Property: Non-zero values in trend should match the selected 5 historical velocities
        # The function takes up to 5 completed sprints. Due to same creation timestamps,
        # it effectively takes the first 5 sprints created (by ID order)
        if num_historical_sprints > 5:
            # Take the first 5 velocities (first 5 sprints created)
            selected_5_velocities = historical_velocities[:5]
        else:
            selected_5_velocities = historical_velocities[:num_historical_sprints]
        
        non_zero_trend = [v for v in velocity_trend if v > 0]
        non_zero_selected_5 = [v for v in selected_5_velocities if v > 0]
        
        # The trend should contain the same non-zero values as the selected 5 historical
        assert len(non_zero_trend) == len(non_zero_selected_5), (
            f"Expected {len(non_zero_selected_5)} non-zero trend values from selected 5 sprints, got {len(non_zero_trend)}. "
            f"Velocity trend: {velocity_trend}, Selected 5: {selected_5_velocities}, "
            f"Non-zero trend: {non_zero_trend}, Non-zero selected: {non_zero_selected_5}"
        )