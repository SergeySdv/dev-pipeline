"""
DevGodzilla Discovery Agent Service

Runs repository discovery via an AI engine (typically `opencode`) using prompt files
that instruct the agent to write durable artifacts into the repo.
"""

from __future__ import annotations

import os
import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Optional

from devgodzilla.engines import EngineNotFoundError, EngineRequest, SandboxMode, get_registry
from devgodzilla.logging import get_logger
from devgodzilla.services.base import Service, ServiceContext
from devgodzilla.services.agent_config import AgentConfigService
from devgodzilla.services.cli_execution_tracker import ExecutionStatus, get_execution_tracker
from devgodzilla.spec import resolve_spec_path

logger = get_logger(__name__)


@dataclass
class DiscoveryStageResult:
    stage: str
    prompt_path: Path
    success: bool
    stdout: str = ""
    stderr: str = ""
    error: Optional[str] = None


@dataclass
class DiscoveryResult:
    success: bool
    engine_id: str
    model: Optional[str]
    repo_root: Path
    log_path: Path
    stages: list[DiscoveryStageResult] = field(default_factory=list)
    expected_outputs: list[Path] = field(default_factory=list)
    missing_outputs: list[Path] = field(default_factory=list)
    error: Optional[str] = None
    fallback_engine_id: Optional[str] = None  # Set when a fallback engine was used
    warning: Optional[str] = None  # Non-fatal warning message


def _resolve_prompt(repo_root: Path, *, prompt_name: str) -> Path:
    repo_local = repo_root / "prompts" / prompt_name
    if repo_local.is_file():
        return repo_local
    fallback = Path(__file__).resolve().parents[2] / "prompts" / prompt_name
    return fallback


class DiscoveryAgentService(Service):
    def __init__(self, context: ServiceContext) -> None:
        super().__init__(context)

    @staticmethod
    def _write_fallback_outputs(*, repo_root: Path, pipeline: bool) -> None:
        """
        When we fall back to a no-op engine (e.g. DummyEngine), create minimal discovery
        artifacts so downstream tooling can proceed in dev/test environments.
        """
        runtime_dir = repo_root / "specs" / "discovery" / "_runtime"
        runtime_dir.mkdir(parents=True, exist_ok=True)

        summary_path = runtime_dir / "DISCOVERY_SUMMARY.json"
        if not summary_path.exists():
            summary_path.write_text(
                json.dumps(
                    {
                        "languages": [],
                        "note": "Generated by DevGodzilla fallback discovery (no-op engine).",
                    },
                    indent=2,
                    sort_keys=True,
                ),
                encoding="utf-8",
            )

        md_defaults = {
            "DISCOVERY.md": "# Discovery\n\nGenerated by fallback discovery (no-op engine).\n",
            "ARCHITECTURE.md": "# Architecture\n\nGenerated by fallback discovery (no-op engine).\n",
            "API_REFERENCE.md": "# API Reference\n\nGenerated by fallback discovery (no-op engine).\n",
            "CI_NOTES.md": "# CI Notes\n\nGenerated by fallback discovery (no-op engine).\n",
        }
        for filename, content in md_defaults.items():
            path = runtime_dir / filename
            if not path.exists():
                path.write_text(content, encoding="utf-8")

        if not pipeline:
            # Non-pipeline mode doesn't expect a summary JSON, but it's harmless to keep it.
            return

    def run_discovery(
        self,
        *,
        repo_root: Path,
        engine_id: str = "opencode",
        model: Optional[str] = None,
        pipeline: bool = True,
        stages: Optional[list[str]] = None,
        timeout_seconds: int = 900,
        strict_outputs: bool = True,
        project_id: Optional[int] = None,
    ) -> DiscoveryResult:
        repo_root = repo_root.expanduser().resolve()
        runtime_dir = self._ensure_discovery_runtime_dir(repo_root)
        log_path = runtime_dir / "opencode-discovery.log"

        stage_map = (
            {
                "inventory": "discovery-inventory.prompt.md",
                "architecture": "discovery-architecture.prompt.md",
                "api_reference": "discovery-api-reference.prompt.md",
                "ci_notes": "discovery-ci-notes.prompt.md",
            }
            if pipeline
            else {"repo_discovery": "repo-discovery.prompt.md"}
        )

        selected = list(stage_map.keys()) if stages is None else stages

        registry = get_registry()
        original_engine_id = engine_id
        fallback_used = False
        
        try:
            engine = registry.get(engine_id)
        except EngineNotFoundError as e:
            return DiscoveryResult(
                success=False,
                engine_id=engine_id,
                model=model,
                repo_root=repo_root,
                log_path=log_path,
                error=f"Engine not registered: {e}",
            )

        if not engine.check_availability():
            # Try to find a fallback engine
            fallback_engines = ["dummy"]  # dummy always available for dev/testing
            for fallback_id in fallback_engines:
                try:
                    fallback = registry.get(fallback_id)
                    if fallback.check_availability():
                        logger.warning(
                            "discovery_engine_fallback",
                            extra={
                                "requested_engine": engine_id,
                                "fallback_engine": fallback_id,
                                "reason": "requested engine unavailable",
                            },
                        )
                        engine = fallback
                        engine_id = fallback_id
                        fallback_used = True
                        break
                except Exception:
                    continue
            
            if not fallback_used:
                return DiscoveryResult(
                    success=False,
                    engine_id=original_engine_id,
                    model=model,
                    repo_root=repo_root,
                    log_path=log_path,
                    error=f"Engine unavailable: {original_engine_id}. No fallback engine available.",
                )

        env_model: Optional[str] = None
        # Environment override wins over config defaults (useful for CI / forced local runs).
        if model is None and engine.metadata.id == "opencode":
            candidate = os.environ.get("DEVGODZILLA_OPENCODE_MODEL")
            if isinstance(candidate, str) and candidate.strip():
                env_model = candidate.strip()

        resolved_agent_model: Optional[str] = None
        if model is None and project_id is not None:
            # Prefer the configured agent default model (UI writes to config/agents.yaml / project overrides)
            # when the caller doesn't explicitly specify a model.
            try:
                cfg = AgentConfigService(self.context)
                agent_cfg = cfg.get_agent(engine_id, project_id=project_id)
                if agent_cfg and isinstance(agent_cfg.default_model, str) and agent_cfg.default_model.strip():
                    resolved_agent_model = agent_cfg.default_model.strip()
            except Exception:
                resolved_agent_model = None

        run_model = model or env_model or resolved_agent_model or engine.metadata.default_model

        # Start CLI execution tracking
        tracker = get_execution_tracker()
        execution = tracker.start_execution(
            execution_type="discovery",
            engine_id=engine_id,
            project_id=project_id,
            command=f"discovery pipeline={pipeline} stages={selected}",
            working_dir=str(repo_root),
            metadata={
                "pipeline": pipeline,
                "stages": selected,
                "model": run_model,
                "original_engine_id": original_engine_id,
                "fallback_used": fallback_used,
            },
        )

        self.logger.info(
            "discovery_agent_start",
            extra=self.log_extra(
                repo_root=str(repo_root),
                engine_id=engine_id,
                model=run_model,
                pipeline=pipeline,
                execution_id=execution.execution_id,
            ),
        )

        tracker.log(execution.execution_id, "info", f"Starting discovery with engine {engine_id}, model {run_model}")
        if fallback_used:
            tracker.log(execution.execution_id, "warn", f"Using fallback engine {engine_id} (original {original_engine_id} unavailable)")

        def _execution_cancelled() -> bool:
            tracked = tracker.get_execution(execution.execution_id)
            return bool(tracked and tracked.status == ExecutionStatus.CANCELLED)

        results: list[DiscoveryStageResult] = []
        for stage in selected:
            if _execution_cancelled():
                tracker.log(
                    execution.execution_id,
                    "warn",
                    f"Discovery cancelled before stage {stage}; stopping remaining stages",
                    source="tracker",
                )
                break
            prompt_name = stage_map.get(stage)
            if not prompt_name:
                results.append(
                    DiscoveryStageResult(
                        stage=stage,
                        prompt_path=Path("<unknown>"),
                        success=False,
                        error=f"Unknown stage: {stage}",
                    )
                )
                continue

            prompt_path = _resolve_prompt(repo_root, prompt_name=prompt_name)
            try:
                cfg = AgentConfigService(self.context)
                assignment = cfg.resolve_prompt_assignment(f"discovery.{stage}", project_id=project_id)
                if not assignment:
                    assignment = cfg.resolve_prompt_assignment("discovery", project_id=project_id)
                if assignment and assignment.get("path"):
                    candidate = resolve_spec_path(str(assignment["path"]), repo_root, repo_root)
                    if candidate.exists():
                        prompt_path = candidate
                    else:
                        self.logger.warning(
                            "discovery_prompt_assignment_missing",
                            extra=self.log_extra(
                                project_id=project_id,
                                prompt_path=str(candidate),
                                stage=stage,
                            ),
                        )
            except Exception:
                prompt_path = prompt_path
            if not prompt_path.is_file():
                results.append(
                    DiscoveryStageResult(
                        stage=stage,
                        prompt_path=prompt_path,
                        success=False,
                        error=f"Prompt missing: {prompt_name}",
                    )
                )
                continue

            prompt_text = prompt_path.read_text(encoding="utf-8")
            
            # Log stage start
            tracker.log(execution.execution_id, "info", f"Executing stage: {stage}", source=engine_id, metadata={"prompt": prompt_name})
            
            streamed_output = False
            def _log_output(source: str, line: str, *, _stage: str = stage) -> None:
                nonlocal streamed_output
                message = line.rstrip("\n")
                if not message:
                    return
                streamed_output = True
                level = "info" if source == "stdout" else "warn"
                tracker.log(
                    execution.execution_id,
                    level,
                    message,
                    source=f"{engine_id}:{source}",
                    metadata={"stage": _stage},
                )

            req = EngineRequest(
                project_id=None,
                protocol_run_id=None,
                step_run_id=None,
                model=run_model,
                prompt_text=prompt_text,
                prompt_files=[str(prompt_path)],
                working_dir=str(repo_root),
                sandbox=SandboxMode.WORKSPACE_WRITE,
                timeout=timeout_seconds,
                extra={
                    "output_format": "text",
                    "job_id": "discovery",
                    "log_callback": _log_output,
                    "cli_execution_id": execution.execution_id,
                },
            )
            engine_result = engine.execute(req)

            # Log stage result to tracker
            if engine_result.success:
                tracker.log(execution.execution_id, "info", f"Stage {stage} completed successfully", source=engine_id)
            else:
                tracker.log(execution.execution_id, "error", f"Stage {stage} failed: {engine_result.error or 'unknown error'}", source=engine_id)
            
            # Log stdout/stderr output (truncated for large outputs)
            if engine_result.stdout and not streamed_output:
                output_preview = engine_result.stdout[:1000] + ("..." if len(engine_result.stdout) > 1000 else "")
                tracker.log(execution.execution_id, "debug", f"[{stage}] stdout: {output_preview}", source="stdout")
            if engine_result.stderr and not streamed_output:
                stderr_preview = engine_result.stderr[:500] + ("..." if len(engine_result.stderr) > 500 else "")
                tracker.log(execution.execution_id, "warn", f"[{stage}] stderr: {stderr_preview}", source="stderr")

            # Best-effort aggregated log for debugging.
            try:
                with log_path.open("a", encoding="utf-8") as f:
                    f.write(f"\n\n===== discovery stage: {stage} ({prompt_name}) =====\n")
                    if engine_result.stdout:
                        f.write(engine_result.stdout)
                    if engine_result.stderr:
                        f.write("\n[stderr]\n")
                        f.write(engine_result.stderr)
            except Exception:
                pass

            results.append(
                DiscoveryStageResult(
                    stage=stage,
                    prompt_path=prompt_path,
                    success=engine_result.success,
                    stdout=engine_result.stdout,
                    stderr=engine_result.stderr,
                    error=engine_result.error,
                )
            )

            if _execution_cancelled():
                tracker.log(
                    execution.execution_id,
                    "warn",
                    f"Discovery cancelled after stage {stage}; stopping remaining stages",
                    source="tracker",
                )
                break

        if fallback_used:
            # Ensure expected output files exist so strict output validation can pass.
            # This is specifically for environments without a real agent CLI installed.
            try:
                self._write_fallback_outputs(repo_root=repo_root, pipeline=pipeline)
            except Exception as exc:
                tracker.log(
                    execution.execution_id,
                    "warn",
                    f"Failed to write fallback discovery outputs: {exc}",
                    source="fallback",
                )

        expected = self._expected_outputs(pipeline=pipeline)
        missing = [p for p in expected if not (repo_root / p).exists()]

        success = all(r.success for r in results) and (not missing if strict_outputs else True)
        error = None
        if not success:
            if _execution_cancelled():
                error = "Discovery cancelled"
            elif missing and strict_outputs:
                error = f"Missing discovery outputs: {', '.join(str(p) for p in missing)}"
            else:
                error = "Discovery failed"

        # Generate warning if we fell back to a different engine
        warning = None
        fallback_engine = None
        if fallback_used:
            fallback_engine = engine_id
            warning = f"Used fallback engine '{engine_id}' instead of requested '{original_engine_id}' (no-op mode)"
            engine_id = original_engine_id  # Report the originally requested engine
        
        # Complete CLI execution tracking
        stages_succeeded = sum(1 for r in results if r.success)
        stages_failed = sum(1 for r in results if not r.success)
        
        if success:
            tracker.log(execution.execution_id, "info", f"Discovery completed successfully: {stages_succeeded}/{len(results)} stages passed")
        else:
            tracker.log(execution.execution_id, "error", f"Discovery failed: {stages_failed}/{len(results)} stages failed, error: {error}")
            if missing:
                tracker.log(execution.execution_id, "warn", f"Missing outputs: {', '.join(str(p) for p in missing)}")
        
        tracker.complete(execution.execution_id, success=success, error=error)
        
        return DiscoveryResult(
            success=success,
            engine_id=engine_id,
            model=run_model,
            repo_root=repo_root,
            log_path=log_path,
            stages=results,
            expected_outputs=[repo_root / p for p in expected],
            missing_outputs=[repo_root / p for p in missing],
            error=error,
            fallback_engine_id=fallback_engine,
            warning=warning,
        )

    def _expected_outputs(self, *, pipeline: bool) -> list[Path]:
        base = Path("specs") / "discovery" / "_runtime"
        if pipeline:
            return [
                base / "DISCOVERY.md",
                base / "DISCOVERY_SUMMARY.json",
                base / "ARCHITECTURE.md",
                base / "API_REFERENCE.md",
                base / "CI_NOTES.md",
            ]
        return [
            base / "DISCOVERY.md",
            base / "ARCHITECTURE.md",
            base / "API_REFERENCE.md",
            base / "CI_NOTES.md",
        ]

    @staticmethod
    def _ensure_discovery_runtime_dir(repo_root: Path) -> Path:
        runtime_dir = repo_root / "specs" / "discovery" / "_runtime"
        runtime_dir.mkdir(parents=True, exist_ok=True)
        return runtime_dir


def parse_discovery_summary(path: Path) -> dict[str, Any]:
    data = json.loads(path.read_text(encoding="utf-8"))
    if not isinstance(data, dict):
        raise ValueError("DISCOVERY_SUMMARY.json must be a JSON object")
    return data
